<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      publications | Charles (Chase) Block
    
  
</title>
<meta name="author" content="Chase Block">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%8A&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/publications/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          Charles (Chase) Block
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/publications/">publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <!-- _pages/publications.md -->

<!-- Bibsearch Feature -->

<script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script>

<p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p>

<div class="publications">

<h2 class="bibliography">2025</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">EXAIT</abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sudusinghe2025automated" class="col-sm-8">
    <!-- Title -->
    <div class="title">Automated Data Selection for Efficient Cost Model Training to Optimize Sparse Matrix Kernels on Emerging Hardware Accelerators</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://chamika2.web.illinois.edu/" rel="external nofollow noopener" target="_blank">Chamika Sudusinghe</a>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://scholar.google.com/citations?user=TliTjHMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Damitha Lenadora</a>, <em>Charles Block</em>, <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>, andÂ <a href="https://charithmendis.com/" rel="external nofollow noopener" target="_blank">Charith Mendis</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Exploration in AI Today Workshop at ICML 2025</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://openreview.net/pdf?id=GdKc5XcO9e" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Sparse matrix computations are critical for many applications in machine learning, computer vision, and scientific computing. However, optimizing sparse kernels, such as Sparse Matrix-Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM), remain challenging as their performance is sensitive to input characteristics and high dimensionality of the scheduling search space. Specifically, this complexity arises from the interplay of factors such as matrix dimensions, sparsity patterns, sparse storage formats, hardware targets, and compiler-specific scheduling primitives, which together create a highly irregular and non-intuitive performance landscape. While prior work has introduced learned cost models to guide the selection of scheduling primitives, these cost models are typically kernel- and hardware-specific, and either require millions of training samples or depend heavily on expert-designed heuristics. In this work, we frame optimizing sparse matrix kernels as a structured exploration problem and identify key limitations in prior work, including its inability to generalize across kernels and hardware, and to train cost models with limited data samples without relying on expert heuristics. We then propose a solution to automate the data collection effort for cost model training on emerging hardware accelerators. Our method augments a state-of-the-art (SOTA) framework with exploration-aware data sampling and multi-armed bandit-based active learning, enabling data-efficient fine-tuning with minimal manual interventions. Our experimental results demonstrate that these strategies substantially reduce reliance on large training datasets and expert heuristics, while achieving performance comparable to SOTA.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sudusinghe2025automated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Data Selection for Efficient Cost Model Training to Optimize Sparse Matrix Kernels on Emerging Hardware Accelerators}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sudusinghe, Chamika and Gerogiannis, Gerasimos and Lenadora, Damitha and Block, Charles and Torrellas, Josep and Mendis, Charith}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Exploration in AI Today Workshop at ICML 2025}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=GdKc5XcO9e}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#440000">
            
              <a href="https://icml.cc" rel="external nofollow noopener" target="_blank">ICML</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sudusinghe2025cognate" class="col-sm-8">
    <!-- Title -->
    <div class="title">COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://chamika2.web.illinois.edu/" rel="external nofollow noopener" target="_blank">Chamika Sudusinghe</a>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://scholar.google.com/citations?user=TliTjHMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Damitha Lenadora</a>, <em>Charles Block</em>, <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>, andÂ <a href="https://charithmendis.com/" rel="external nofollow noopener" target="_blank">Charith Mendis</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Forty-second International Conference on Machine Learning</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://openreview.net/pdf?id=EV0itGFjmm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47Ã (up to 5.46Ã) for SpMM and 1.39Ã (up to 4.22Ã) for SDDMM.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sudusinghe2025cognate</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{COGNATE}: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sudusinghe, Chamika and Gerogiannis, Gerasimos and Lenadora, Damitha and Block, Charles and Torrellas, Josep and Mendis, Charith}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=EV0itGFjmm}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">Cell</abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="vasconcelos24scalp" class="col-sm-8">
    <!-- Title -->
    <div class="title">On-scalp printing of personalized electroencephalography e-tattoos</div>
    <!-- Author -->
    <div class="author">
      

      
      Luize Scalco de Vasconcelos, Yichen Yan, Pukar Maharjan, Satyam Kumar, Minsu Zhang, Bowen Yao, Hongbian Li, Sidi Duan, Eric Li, Eric Williams, Sandhya Tiku, Pablo Vidal, and
        <span class="more-authors" title="click to view 13 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '13 more authors' ? 'R. Sergio Solorzano-Vargas, Wen Hong, Yingjie Du, Zixiao Liu, Fumiaki Iwane, &lt;em&gt;Charles Block&lt;/em&gt;, Andrew T. Repetski, Philip Tan, Pulin Wang, MartÃ­n G. MartÃ­n, JosÃ© del R. MillÃ¡n, Ximin He, Nanshu Lu' : '13 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">13 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Cell Biomaterials</em>,  Dec 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://www.cell.com/cell-biomaterials/pdfExtended/S3050-5623(24)00004-7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>On-scalp digital printing of custom-designed, temporary-tattoo-like sensors represents a groundbreaking advancement in noninvasive brain-monitoring technologies, advancing the fields of neuroscience, clinical diagnostics, and brain-computer interfaces (BCIs). Traditional electroencephalography (EEG) systems involve time-consuming manual electrode placement, conductive liquid gels, and cumbersome cables, which are prone to signal degradation and discomfort during prolonged use. Our approach overcomes these limitations by combining material innovations with non-contact, on-body digital printing techniques to fabricate e-tattoos that are self-drying, ultrathin, and compatible with hairy scalps. These skin-conformal EEG e-tattoo sensors enable comfortable, long-term, high-quality brain activity monitoring without the discomfort associated with traditional EEG systems. Using individual 3D head scans, custom sensor layout design, and a 5-axis microjet printing robot, we have created EEG e-tattoos with precise, tailored placement over the entire scalp. The inks for electrodes and interconnects have slightly different compositions to achieve low skin contact impedance and high bulk conductivity, respectively. After printing and self-drying, the inks form conductive, stretchable, and breathable thin films that ensure high signal fidelity, even over extended periods. This technology paves the way for non-invasive, high-performance, and user-friendly brain monitoring that will enhance both patient care and the understanding of the human brain. The broader significance of this technology lies in its potential applications beyond traditional EEG use. On-scalp printed ultrathin e-tattoos could play a pivotal role in developing BCIs for various industries, including prosthetics, virtual reality (VR), and human-robot teaming. This work also opens the possibility of on-body digital manufacture of other types of e-tattoo devices in areas beyond the head, leading to large-area, skin-covered yet deformable and breathable functional e-tattoos.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vasconcelos24scalp</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{Scalco de Vasconcelos}, Luize and Yan, Yichen and Maharjan, Pukar and Kumar, Satyam and Zhang, Minsu and Yao, Bowen and Li, Hongbian and Duan, Sidi and Li, Eric and Williams, Eric and Tiku, Sandhya and Vidal, Pablo and Solorzano-Vargas, R. Sergio and Hong, Wen and Du, Yingjie and Liu, Zixiao and Iwane, Fumiaki and Block, Charles and Repetski, Andrew T. and Tan, Philip and Wang, Pulin and MartÃ­n, MartÃ­n G. and MillÃ¡n, JosÃ© del R. and He, Ximin and Lu, Nanshu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On-scalp printing of personalized electroencephalography e-tattoos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Cell Biomaterials}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.celbio.2024.100004}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{3050-5623}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#444444">
            
              <a href="https://supercomputing.org/" rel="external nofollow noopener" target="_blank">SC</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="ranawaka24sc" class="col-sm-8">
    <!-- Title -->
    <div class="title">Distributed-Memory Parallel Algorithms for Sparse Matrix and Sparse Tall-and-Skinny Matrix Multiplication</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://scholar.google.com/citations?user=J258nmAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Isuru Ranawaka</a>, <a href="https://scholar.google.com/citations?user=RuDNWj8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Md Taufique Hussain</a>, <em>Charles Block</em>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>, andÂ <a href="https://scholar.google.com/citations?user=9jK5lfsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ariful Azad</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis</em>, Atlanta, GA, USA,  Nov 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://arxiv.org/pdf/2408.11988" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We consider a sparse matrix-matrix multiplication (SpGEMM) setting where one matrix is square and the other is tall and skinny. This special variant, TS-SpGEMM, has important applications in multi-source breadth-first search, influence maximization, sparse graph embedding, and algebraic multigrid solvers. Unfortunately, popular distributed algorithms like sparse SUMMA deliver suboptimal performance for TS-SpGEMM. To address this limitation, we develop a novel distributed-memory algorithm tailored for TS-SpGEMM. Our approach employs customized 1D partitioning for all matrices involved and leverages sparsity-aware tiling for efficient data transfers. In addition, it minimizes communication overhead by incorporating both local and remote computations. On average, our TS-SpGEMM algorithm attains 5\texttimes performance gains over 2D and 3D SUMMA. Furthermore, we use our algorithm to implement multi-source breadth-first search and sparse graph embedding algorithms and demonstrate their scalability up to 512 Nodes (or 65,536 cores) on NERSC Perlmutter.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ranawaka24sc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ranawaka, Isuru and Hussain, Md Taufique and Block, Charles and Gerogiannis, Gerasimos and Torrellas, Josep and Azad, Ariful}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Distributed-Memory Parallel Algorithms for Sparse Matrix and Sparse Tall-and-Skinny Matrix Multiplication}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798350352917}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE Press}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/SC41406.2024.00052}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/SC41406.2024.00052}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Atlanta, GA, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SC '24}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">
            
              <a href="https://dl.acm.org/conference/pact" rel="external nofollow noopener" target="_blank">PACT</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="suresh24pact" class="col-sm-8">
    <!-- Title -->
    <div class="title">Mozart: Taming Taxes and Composing Accelerators with Shared-Memory</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://scholar.google.com/citations?user=rFE8RPEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Vignesh Suresh</a>, <a href="http://bakshree.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Bakshree Mishra</a>, <a href="https://yingj4.github.io/" rel="external nofollow noopener" target="_blank">Ying Jing</a>, Zeran Zhu, <a href="https://www.linkedin.com/in/naiyin-jin-09737521a/" rel="external nofollow noopener" target="_blank">Naiyin Jin</a>, <em>Charles Block</em>, <a href="https://www.cs.columbia.edu/~paolo/aboutme/" rel="external nofollow noopener" target="_blank">Paolo Mantovani</a>, <a href="https://www.cs.columbia.edu/~davide_giri/" rel="external nofollow noopener" target="_blank">Davide Giri</a>, <a href="http://www.cs.columbia.edu/~jzuck/" rel="external nofollow noopener" target="_blank">Joseph Zuckerman</a>, <a href="http://www.cs.columbia.edu/~luca/" rel="external nofollow noopener" target="_blank">Luca P. Carloni</a>, andÂ Sarita V. Adve
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques</em>, Long Beach, CA, USA,  Oct 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://rsim.cs.illinois.edu/Pubs/24-PACT-Mozart.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Resource-constrained system-on-chips (SoCs) are increasingly heterogeneous with specialized accelerators for various tasks. Acceleration taxes due to control and data movement, however, diminish end-to-end speedups from hardware acceleration. Meanwhile, emerging workloads are increasingly task-diverse with several, potentially shared, fine-grained acceleration candidates. This motivates a paradigm of parallel and disaggregated acceleration. Compared to a monolithic accelerator, disaggregation provides higher flexibility, reuse, and utilization, but at the cost of higher control and data acceleration taxes. We propose a novel SoC architecture, Mozart, that enables efficient accelerator disaggregation by leveraging shared-memory to tame control and data acceleration taxes. To address the control tax, Mozart includes a lightweight, modular, and general accelerator synchronization interface (ASI). ASI eliminates the typical CPU-centric accelerator control in favor of a decentralized, uniform synchronization interface through shared-memory. This enables accelerators to directly and transparently synchronize with each other (or CPUs) using the same shared-memory interface as CPUs. To address the data tax, Mozart leverages the Spandex-FCS heterogeneous coherence protocol, which supports decentralized data movement and per-word coherence specialization. We demonstrate the first RTL implementation of Spandex-FCS and the first evaluation of its benefits for a heterogeneous SoC with fixed-function accelerators, running real-world applications with Linux. Mozart simultaneously enables, for the first time, (1) finer-grained acceleration than previously possible, (2) programmable and transparent composition of fine-grained, disaggregated accelerators, (3) efficient accelerator pipelining through shared-memory and decentralization, and (4) a performance-competitive disaggregated alternative to specialized monolithic accelerators. We demonstrate these capabilities of Mozart with a comprehensive one-of-a-kind evaluation of more than 70 hardware configurations prototyped on an FPGA employing various accelerators, running real-world applications on Linux, and a scalability analysis with up to 15 accelerators. We also present an analytical performance model to understand and explore system design choices and to validate the results.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">suresh24pact</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Suresh, Vignesh and Mishra, Bakshree and Jing, Ying and Zhu, Zeran and Jin, Naiyin and Block, Charles and Mantovani, Paolo and Giri, Davide and Zuckerman, Joseph and Carloni, Luca P. and Adve, Sarita V.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mozart: Taming Taxes and Composing Accelerators with Shared-Memory}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400706318}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3656019.3676896}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3656019.3676896}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{183â200}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Accelerator Synchronization, Cache Coherence, Disaggregated Acceleration, Heterogeneous Systems, Shared-Memory}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Long Beach, CA, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{PACT '24}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">
            
              <a href="https://www.sciencedirect.com/journal/matter" rel="external nofollow noopener" target="_blank">Matter</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="ha24shrps" class="col-sm-8">
    <!-- Title -->
    <div class="title">Stretchable hybrid response pressure sensors</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://scholar.google.com/citations?hl=en&amp;user=lMju4RMAAAAJ" rel="external nofollow noopener" target="_blank">Kyoung-Ho Ha</a>, <a href="https://scholar.google.com/citations?user=ywbBVVUAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Zhengjie Li</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=49SqE-0AAAAJ" rel="external nofollow noopener" target="_blank">Sangjun Kim</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=A7FxTBAAAAAJ" rel="external nofollow noopener" target="_blank">Heeyong Huh</a>, Zheliang Wang, <a href="https://scholar.google.com/citations?hl=en&amp;user=JqsZ5A4AAAAJ" rel="external nofollow noopener" target="_blank">Hongyang Shi</a>, <em>Charles Block</em>, <a href="https://scholar.google.com/citations?user=O9HdCi0AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sarnab Bhattacharya</a>, andÂ <a href="https://sites.utexas.edu/nanshulu/" rel="external nofollow noopener" target="_blank">Nanshu Lu</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Matter</em>,  May 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Touch-sensitive stretchable electronic skins (e-skins) hold promise for soft robots, prosthetics, bio-mimetics, and bio-sensors. However,Â a long-standing challenge has been the interference of stretchingÂ in pressure readings. Addressing this, we introduce an intrinsically stretchable hybrid response pressure sensor (SHRPS) composed of a laminate featuring a barely conductive porous nanocomposite and an ultrathin dielectric layer situated between two stretchable electrodes. The combined piezoresistive and piezocapacitive responses of the SHRPS enable ultrahigh pressure sensitivity while effectively negating stretch-induced interference. Our findings are underpinned by an experimentally validated electromechanical model. In practical applications, SHRPS mounted on inflatable probes demonstrated safe and precise palpation on the human wrist and conformable and firm gripping of contoured objects. The debut of SHRPS promises to significantly expand the versatile applications of e-skins.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ha24shrps</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stretchable hybrid response pressure sensors}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Matter}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1895-1908}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2590-2385}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.matt.2024.04.009}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2590238524001644}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ha, Kyoung-Ho and Li, Zhengjie and Kim, Sangjun and Huh, Heeyong and Wang, Zheliang and Shi, Hongyang and Block, Charles and Bhattacharya, Sarnab and Lu, Nanshu}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{stretchable pressure sensor, electronic skin, decoupling of pressure and stretch, inflatable soft robots, human-computer interaction, heartbeat palpation}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#00aa00">
            
              <a href="https://www.asplos-conference.org/" rel="external nofollow noopener" target="_blank">ASPLOS</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="block24twoface" class="col-sm-8">
    <!-- Title -->
    <div class="title">Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Charles Block</em>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://charithmendis.com/" rel="external nofollow noopener" target="_blank">Charith Mendis</a>, <a href="https://scholar.google.com/citations?user=9jK5lfsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ariful Azad</a>, andÂ <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, La Jolla, CA, USA,  Apr 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="/assets/pdf/asplos24_3.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Sparse matrix dense matrix multiplication (SpMM) is commonly used in applications ranging from scientific computing to graph neural networks. Typically, when SpMM is executed in a distributed platform, communication costs dominate. Such costs depend on how communication is scheduled. If it is scheduled in a sparsity-unaware manner, such as with collectives, execution is often inefficient due to unnecessary data transfers. On the other hand, if communication is scheduled in a fine-grained sparsity-aware manner, communicating only the necessary data, execution can also be inefficient due to high software overhead.We observe that individual sparse matrices often contain regions that are denser and regions that are sparser. Based on this observation, we develop a model that partitions communication into sparsity-unaware and sparsity-aware components. Leveraging the partition, we develop a new algorithm that performs collective communication for the denser regions, and fine-grained, one-sided communication for the sparser regions. We call the algorithm Two-Face. We show that Two-Face attains an average speedup of 2.11x over prior work when evaluated on a 4096-core supercomputer. Additionally, Two-Face scales well with the machine size.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">block24twoface</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Block, Charles and Gerogiannis, Gerasimos and Mendis, Charith and Azad, Ariful and Torrellas, Josep}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703850}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3620665.3640427}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3620665.3640427}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1200â1217}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{high-performance computing, distributed algorithms, sparse matrices, SpMM}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{La Jolla, CA, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ASPLOS '24}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>

</div>

  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      Â© Copyright 2025
      Chase
      
      Block. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-S3WKFCL66D"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      window.dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-S3WKFCL66D');
  </script>




    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
