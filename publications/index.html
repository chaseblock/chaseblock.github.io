<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      publications | Charles (Chase) Block
    
  
</title>
<meta name="author" content="Chase Block">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BE&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://chaseblock.com//publications/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          Charles (Chase) Block
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/publications/">publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <!-- _pages/publications.md -->

<!-- Bibsearch Feature -->

<script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script>

<p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p>

<div class="publications">

<h2 class="bibliography">2026</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">DPC4</abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="block26" class="col-sm-8">
    <!-- Title -->
    <div class="title">Performance-Driven Composite Prefetching with Bandits</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Charles Block</em>, Pedro Palacios Almendros, Abraham Farrell, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, and <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Fourth Data Prefetching Champsionship at HPCA 2026</em>, Sydney, Australia,  Feb 2026
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="/assets/pdf/dpc4.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Data prefetching is a complex problem. Its complexity has spurred research advancements for over half a century. Our submission to DPC4 is grounded on two observations. First, we observe that prefetchers are typically designed to optimize different low-level metrics such as timeliness, accuracy, and coverage. However, the correlation between those low-level metrics and the actual performance impact is complex and many times fluctuates depending on the application and system status. Second, we observe that industry often integrates ensembles of smaller prefetchers instead of large monoliths in the cache hierarchy of real products. Motivated by those observations, we (1) split the available competition storage budget across different sub-prefetchers at different cache levels, and we (2) use performance-driven reinforcement-learning (RL) agents to manage and coordinate the activity of our composite prefetchers. Our RL agents are based on the Multi-Armed Bandit (MAB) model, and instead of optimizing for one or more low-level metrics, they are configured to directly maximize performance. Further, to improve the fairness of the prefetchers in multicore configurations, we apply a version of the Micro-MAMA arbitrator, which also relies on Multi-Armed Bandit forms of Reinforcement Learning. Compared to the original MAB and μMama works, we use a wider selection of lightweight prefetchers in our RL-managed ensemble at the L2, combined with Berti at the L1. In this submission, we discuss the ensemble of prefetchers used in our design, as well as the coordination mechanisms used between them. Our Bandit-based composite prefetcher achieves strong single-core performance in the two system configurations evaluated in this competition, with geomean speedups of 2.7% and 1.2% over the baseline system in the Full- and Limited-Bandwidth cases, respectively. We conclude by discussing learnings we acquired during the design and tuning process that we believe can prove useful for the design space exploration and evaluation of future prefetchers.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">block26</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Performance-Driven Composite Prefetching with Bandits}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Block, Charles and Palacios Almendros, Pedro and Farrell, Abraham and Gerogiannis, Gerasimos and Torrellas, Josep}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Fourth Data Prefetching Champsionship at HPCA 2026}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{DPC4}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Sydney, Australia}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2025</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#0000aa">
            
              <a href="https://www.microarch.org" rel="external nofollow noopener" target="_blank">MICRO</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="block25mama" class="col-sm-8">
    <!-- Title -->
    <div class="title">Micro-MAMA: Multi-Agent Reinforcement Learning for Multicore Prefetching</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Charles Block</em>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, and <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 58th IEEE/ACM International Symposium on Microarchitecture</em>, Seoul, Republic of Korea,  Oct 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="/assets/pdf/micro25.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Online reinforcement learning (RL) holds promise for microarchitectural techniques like prefetching. Its ability to adapt to changing and previously-unseen scenarios makes it a versatile technique. However, when multiple RL-operated components compete for shared resources in multicore systems, they can often converge to sub-optimal policies due to conflicting incentives. In this work, we identify key challenges that arise when scaling RL-based prefetchers to multi-core environments, and relate these to known problems from Multi-Agent Reinforcement Learning (MARL). In particular, we find that recent work using multi-armed bandit algorithms for prefetching can lead to inefficient systems when memory bandwidth is limited, as each agent attempts to claim a disproportionate share of the system’s bandwidth. To solve this problem, we present μMama, a light-weight supervisor of distributed multi-armed bandit agents, which learns performant joint-policies. In μMama, distributed local agents narrow the global joint-action search space, while a central agent with a global perspective learns system-wide policies. Additionally, μMama provides key local agents with a system perspective, encouraging them to avoid actions that would harm the others. μMama exhibits high adaptability, which we show by evaluating it using multiple measures of performance. In our evaluation of an 8-core system, the policies learned by μMama outperform those of independently-operating agents by an average of 2.1% when optimizing for throughput, and by an average of 10.4% when optimizing for fairness. We also show that μMama performs better in systems that are more bandwidth constrained, as well as when profiles of the workloads are provided.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">block25mama</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Micro-MAMA: Multi-Agent Reinforcement Learning for Multicore Prefetching}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Block, Charles and Gerogiannis, Gerasimos and Torrellas, Josep}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{58th IEEE/ACM International Symposium on Microarchitecture}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MICRO '25}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Seoul, Republic of Korea}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3725843.3756096}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3725843.3756096}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{979-8-4007-1573-0/2025/10}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#0000aa">
            
              <a href="https://www.microarch.org" rel="external nofollow noopener" target="_blank">MICRO</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="gerogiannis25netsparse" class="col-sm-8">
    <!-- Title -->
    <div class="title">NetSparse: In-Network Acceleration of Distributed Sparse Kernels</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, Dimitrios Merkouriadis, <em>Charles Block</em>, Annus Zulfiqar, Filippos Tofalos, Muhammad Shahbaz, and <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 58th IEEE/ACM International Symposium on Microarchitecture</em>, Seoul, Republic of Korea,  Oct 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://iacoma.cs.uiuc.edu/iacoma-papers/micro25_1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Many hardware accelerators have been proposed to accelerate sparse computations. When these accelerators are placed in the nodes of a large cluster, distributed sparse applications become heavily communication-bound. Unfortunately, software solutions to optimize network communication are inefficient. In this paper, we introduce novel hardware mechanisms to optimize network communication in distributed sparse computations. Our proposal, called NetSparse, consists of four mechanisms. Communication is offloaded to new processing units in the NIC that support efficient remote indexed gather operations, minimizing host-NIC communication. Moreover, these units have the ability to identify and eliminate redundant requests, which minimizes traffic. Further, new hardware modules in the NICs and switches concatenate multiple requests with the same destination node in a single packet, saving traffic and header overheads. Finally, switches are augmented with a hardware cache that stores fetched data from remote racks, making it available to all the nodes in the local rack on demand. Our evaluation on a simulated 128-node cluster with per-node sparse accelerators running sparse workloads reveals that NetSparse improves performance substantially. When the cluster uses traditional software-based communication, the workloads run only 3x faster than on a single-node system; when it is augmented with the NetSparse hardware, the workloads run 38x faster than on the single-node system—attaining more than half of the performance of an ideal system that has no communication overheads.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gerogiannis25netsparse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NetSparse: In-Network Acceleration of Distributed Sparse Kernels}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gerogiannis, Gerasimos and Merkouriadis, Dimitrios and Block, Charles and Zulfiqar, Annus and Tofalos, Filippos and Shahbaz, Muhammad and Torrellas, Josep}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{58th IEEE/ACM International Symposium on Microarchitecture}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MICRO '25}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Seoul, Republic of Korea}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3725843.3756076}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3725843.3756076}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{979-8-4007-1573-0/25/10}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">EXAIT</abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sudusinghe2025automated" class="col-sm-8">
    <!-- Title -->
    <div class="title">Automated Data Selection for Efficient Cost Model Training to Optimize Sparse Matrix Kernels on Emerging Hardware Accelerators</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://chamika2.web.illinois.edu/" rel="external nofollow noopener" target="_blank">Chamika Sudusinghe</a>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://scholar.google.com/citations?user=TliTjHMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Damitha Lenadora</a>, <em>Charles Block</em>, <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>, and <a href="https://charithmendis.com/" rel="external nofollow noopener" target="_blank">Charith Mendis</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Exploration in AI Today Workshop at ICML 2025</em>,  Jul 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://openreview.net/pdf?id=GdKc5XcO9e" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Sparse matrix computations are critical for many applications in machine learning, computer vision, and scientific computing. However, optimizing sparse kernels, such as Sparse Matrix-Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM), remain challenging as their performance is sensitive to input characteristics and high dimensionality of the scheduling search space. Specifically, this complexity arises from the interplay of factors such as matrix dimensions, sparsity patterns, sparse storage formats, hardware targets, and compiler-specific scheduling primitives, which together create a highly irregular and non-intuitive performance landscape. While prior work has introduced learned cost models to guide the selection of scheduling primitives, these cost models are typically kernel- and hardware-specific, and either require millions of training samples or depend heavily on expert-designed heuristics. In this work, we frame optimizing sparse matrix kernels as a structured exploration problem and identify key limitations in prior work, including its inability to generalize across kernels and hardware, and to train cost models with limited data samples without relying on expert heuristics. We then propose a solution to automate the data collection effort for cost model training on emerging hardware accelerators. Our method augments a state-of-the-art (SOTA) framework with exploration-aware data sampling and multi-armed bandit-based active learning, enabling data-efficient fine-tuning with minimal manual interventions. Our experimental results demonstrate that these strategies substantially reduce reliance on large training datasets and expert heuristics, while achieving performance comparable to SOTA.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sudusinghe2025automated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Data Selection for Efficient Cost Model Training to Optimize Sparse Matrix Kernels on Emerging Hardware Accelerators}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sudusinghe, Chamika and Gerogiannis, Gerasimos and Lenadora, Damitha and Block, Charles and Torrellas, Josep and Mendis, Charith}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Exploration in AI Today Workshop at ICML 2025}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=GdKc5XcO9e}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#440000">
            
              <a href="https://icml.cc" rel="external nofollow noopener" target="_blank">ICML</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sudusinghe2025cognate" class="col-sm-8">
    <!-- Title -->
    <div class="title">COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://chamika2.web.illinois.edu/" rel="external nofollow noopener" target="_blank">Chamika Sudusinghe</a>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://scholar.google.com/citations?user=TliTjHMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Damitha Lenadora</a>, <em>Charles Block</em>, <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>, and <a href="https://charithmendis.com/" rel="external nofollow noopener" target="_blank">Charith Mendis</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Forty-second International Conference on Machine Learning</em>,  Jul 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://openreview.net/pdf?id=EV0itGFjmm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47× (up to 5.46×) for SpMM and 1.39× (up to 4.22×) for SDDMM.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sudusinghe2025cognate</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{COGNATE}: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sudusinghe, Chamika and Gerogiannis, Gerasimos and Lenadora, Damitha and Block, Charles and Torrellas, Josep and Mendis, Charith}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-second International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=EV0itGFjmm}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">Cell</abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="vasconcelos24scalp" class="col-sm-8">
    <!-- Title -->
    <div class="title">On-scalp printing of personalized electroencephalography e-tattoos</div>
    <!-- Author -->
    <div class="author">
      

      
      Luize Scalco de Vasconcelos, Yichen Yan, Pukar Maharjan, Satyam Kumar, Minsu Zhang, Bowen Yao, Hongbian Li, Sidi Duan, Eric Li, Eric Williams, Sandhya Tiku, Pablo Vidal, and
        <span class="more-authors" title="click to view 13 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '13 more authors' ? 'R. Sergio Solorzano-Vargas, Wen Hong, Yingjie Du, Zixiao Liu, Fumiaki Iwane, &lt;em&gt;Charles Block&lt;/em&gt;, Andrew T. Repetski, Philip Tan, Pulin Wang, Martín G. Martín, José del R. Millán, Ximin He, Nanshu Lu' : '13 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">13 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Cell Biomaterials</em>,  Dec 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://www.cell.com/cell-biomaterials/pdfExtended/S3050-5623(24)00004-7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>On-scalp digital printing of custom-designed, temporary-tattoo-like sensors represents a groundbreaking advancement in noninvasive brain-monitoring technologies, advancing the fields of neuroscience, clinical diagnostics, and brain-computer interfaces (BCIs). Traditional electroencephalography (EEG) systems involve time-consuming manual electrode placement, conductive liquid gels, and cumbersome cables, which are prone to signal degradation and discomfort during prolonged use. Our approach overcomes these limitations by combining material innovations with non-contact, on-body digital printing techniques to fabricate e-tattoos that are self-drying, ultrathin, and compatible with hairy scalps. These skin-conformal EEG e-tattoo sensors enable comfortable, long-term, high-quality brain activity monitoring without the discomfort associated with traditional EEG systems. Using individual 3D head scans, custom sensor layout design, and a 5-axis microjet printing robot, we have created EEG e-tattoos with precise, tailored placement over the entire scalp. The inks for electrodes and interconnects have slightly different compositions to achieve low skin contact impedance and high bulk conductivity, respectively. After printing and self-drying, the inks form conductive, stretchable, and breathable thin films that ensure high signal fidelity, even over extended periods. This technology paves the way for non-invasive, high-performance, and user-friendly brain monitoring that will enhance both patient care and the understanding of the human brain. The broader significance of this technology lies in its potential applications beyond traditional EEG use. On-scalp printed ultrathin e-tattoos could play a pivotal role in developing BCIs for various industries, including prosthetics, virtual reality (VR), and human-robot teaming. This work also opens the possibility of on-body digital manufacture of other types of e-tattoo devices in areas beyond the head, leading to large-area, skin-covered yet deformable and breathable functional e-tattoos.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vasconcelos24scalp</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{{Scalco de Vasconcelos}, Luize and Yan, Yichen and Maharjan, Pukar and Kumar, Satyam and Zhang, Minsu and Yao, Bowen and Li, Hongbian and Duan, Sidi and Li, Eric and Williams, Eric and Tiku, Sandhya and Vidal, Pablo and Solorzano-Vargas, R. Sergio and Hong, Wen and Du, Yingjie and Liu, Zixiao and Iwane, Fumiaki and Block, Charles and Repetski, Andrew T. and Tan, Philip and Wang, Pulin and Martín, Martín G. and Millán, José del R. and He, Ximin and Lu, Nanshu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On-scalp printing of personalized electroencephalography e-tattoos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Cell Biomaterials}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.celbio.2024.100004}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{3050-5623}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#444444">
            
              <a href="https://supercomputing.org/" rel="external nofollow noopener" target="_blank">SC</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="ranawaka24sc" class="col-sm-8">
    <!-- Title -->
    <div class="title">Distributed-Memory Parallel Algorithms for Sparse Matrix and Sparse Tall-and-Skinny Matrix Multiplication</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://scholar.google.com/citations?user=J258nmAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Isuru Ranawaka</a>, <a href="https://scholar.google.com/citations?user=RuDNWj8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Md Taufique Hussain</a>, <em>Charles Block</em>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>, and <a href="https://scholar.google.com/citations?user=9jK5lfsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ariful Azad</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis</em>, Atlanta, GA, USA,  Nov 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://arxiv.org/pdf/2408.11988" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We consider a sparse matrix-matrix multiplication (SpGEMM) setting where one matrix is square and the other is tall and skinny. This special variant, TS-SpGEMM, has important applications in multi-source breadth-first search, influence maximization, sparse graph embedding, and algebraic multigrid solvers. Unfortunately, popular distributed algorithms like sparse SUMMA deliver suboptimal performance for TS-SpGEMM. To address this limitation, we develop a novel distributed-memory algorithm tailored for TS-SpGEMM. Our approach employs customized 1D partitioning for all matrices involved and leverages sparsity-aware tiling for efficient data transfers. In addition, it minimizes communication overhead by incorporating both local and remote computations. On average, our TS-SpGEMM algorithm attains 5\texttimes performance gains over 2D and 3D SUMMA. Furthermore, we use our algorithm to implement multi-source breadth-first search and sparse graph embedding algorithms and demonstrate their scalability up to 512 Nodes (or 65,536 cores) on NERSC Perlmutter.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ranawaka24sc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ranawaka, Isuru and Hussain, Md Taufique and Block, Charles and Gerogiannis, Gerasimos and Torrellas, Josep and Azad, Ariful}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Distributed-Memory Parallel Algorithms for Sparse Matrix and Sparse Tall-and-Skinny Matrix Multiplication}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798350352917}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE Press}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/SC41406.2024.00052}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/SC41406.2024.00052}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Atlanta, GA, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SC '24}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">
            
              <a href="https://dl.acm.org/conference/pact" rel="external nofollow noopener" target="_blank">PACT</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="suresh24pact" class="col-sm-8">
    <!-- Title -->
    <div class="title">Mozart: Taming Taxes and Composing Accelerators with Shared-Memory</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://scholar.google.com/citations?user=rFE8RPEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Vignesh Suresh</a>, <a href="http://bakshree.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Bakshree Mishra</a>, <a href="https://yingj4.github.io/" rel="external nofollow noopener" target="_blank">Ying Jing</a>, Zeran Zhu, <a href="https://www.linkedin.com/in/naiyin-jin-09737521a/" rel="external nofollow noopener" target="_blank">Naiyin Jin</a>, <em>Charles Block</em>, <a href="https://www.cs.columbia.edu/~paolo/aboutme/" rel="external nofollow noopener" target="_blank">Paolo Mantovani</a>, <a href="https://www.cs.columbia.edu/~davide_giri/" rel="external nofollow noopener" target="_blank">Davide Giri</a>, <a href="http://www.cs.columbia.edu/~jzuck/" rel="external nofollow noopener" target="_blank">Joseph Zuckerman</a>, <a href="http://www.cs.columbia.edu/~luca/" rel="external nofollow noopener" target="_blank">Luca P. Carloni</a>, and Sarita V. Adve
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques</em>, Long Beach, CA, USA,  Oct 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="https://rsim.cs.illinois.edu/Pubs/24-PACT-Mozart.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Resource-constrained system-on-chips (SoCs) are increasingly heterogeneous with specialized accelerators for various tasks. Acceleration taxes due to control and data movement, however, diminish end-to-end speedups from hardware acceleration. Meanwhile, emerging workloads are increasingly task-diverse with several, potentially shared, fine-grained acceleration candidates. This motivates a paradigm of parallel and disaggregated acceleration. Compared to a monolithic accelerator, disaggregation provides higher flexibility, reuse, and utilization, but at the cost of higher control and data acceleration taxes. We propose a novel SoC architecture, Mozart, that enables efficient accelerator disaggregation by leveraging shared-memory to tame control and data acceleration taxes. To address the control tax, Mozart includes a lightweight, modular, and general accelerator synchronization interface (ASI). ASI eliminates the typical CPU-centric accelerator control in favor of a decentralized, uniform synchronization interface through shared-memory. This enables accelerators to directly and transparently synchronize with each other (or CPUs) using the same shared-memory interface as CPUs. To address the data tax, Mozart leverages the Spandex-FCS heterogeneous coherence protocol, which supports decentralized data movement and per-word coherence specialization. We demonstrate the first RTL implementation of Spandex-FCS and the first evaluation of its benefits for a heterogeneous SoC with fixed-function accelerators, running real-world applications with Linux. Mozart simultaneously enables, for the first time, (1) finer-grained acceleration than previously possible, (2) programmable and transparent composition of fine-grained, disaggregated accelerators, (3) efficient accelerator pipelining through shared-memory and decentralization, and (4) a performance-competitive disaggregated alternative to specialized monolithic accelerators. We demonstrate these capabilities of Mozart with a comprehensive one-of-a-kind evaluation of more than 70 hardware configurations prototyped on an FPGA employing various accelerators, running real-world applications on Linux, and a scalability analysis with up to 15 accelerators. We also present an analytical performance model to understand and explore system design choices and to validate the results.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">suresh24pact</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Suresh, Vignesh and Mishra, Bakshree and Jing, Ying and Zhu, Zeran and Jin, Naiyin and Block, Charles and Mantovani, Paolo and Giri, Davide and Zuckerman, Joseph and Carloni, Luca P. and Adve, Sarita V.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mozart: Taming Taxes and Composing Accelerators with Shared-Memory}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400706318}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3656019.3676896}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3656019.3676896}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{183–200}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Accelerator Synchronization, Cache Coherence, Disaggregated Acceleration, Heterogeneous Systems, Shared-Memory}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Long Beach, CA, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{PACT '24}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">
            
              <a href="https://www.sciencedirect.com/journal/matter" rel="external nofollow noopener" target="_blank">Matter</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="ha24shrps" class="col-sm-8">
    <!-- Title -->
    <div class="title">Stretchable hybrid response pressure sensors</div>
    <!-- Author -->
    <div class="author">
      

      
      <a href="https://scholar.google.com/citations?hl=en&amp;user=lMju4RMAAAAJ" rel="external nofollow noopener" target="_blank">Kyoung-Ho Ha</a>, <a href="https://scholar.google.com/citations?user=ywbBVVUAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Zhengjie Li</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=49SqE-0AAAAJ" rel="external nofollow noopener" target="_blank">Sangjun Kim</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=A7FxTBAAAAAJ" rel="external nofollow noopener" target="_blank">Heeyong Huh</a>, Zheliang Wang, <a href="https://scholar.google.com/citations?hl=en&amp;user=JqsZ5A4AAAAJ" rel="external nofollow noopener" target="_blank">Hongyang Shi</a>, <em>Charles Block</em>, <a href="https://scholar.google.com/citations?user=O9HdCi0AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sarnab Bhattacharya</a>, and <a href="https://sites.utexas.edu/nanshulu/" rel="external nofollow noopener" target="_blank">Nanshu Lu</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Matter</em>,  May 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Touch-sensitive stretchable electronic skins (e-skins) hold promise for soft robots, prosthetics, bio-mimetics, and bio-sensors. However, a long-standing challenge has been the interference of stretching in pressure readings. Addressing this, we introduce an intrinsically stretchable hybrid response pressure sensor (SHRPS) composed of a laminate featuring a barely conductive porous nanocomposite and an ultrathin dielectric layer situated between two stretchable electrodes. The combined piezoresistive and piezocapacitive responses of the SHRPS enable ultrahigh pressure sensitivity while effectively negating stretch-induced interference. Our findings are underpinned by an experimentally validated electromechanical model. In practical applications, SHRPS mounted on inflatable probes demonstrated safe and precise palpation on the human wrist and conformable and firm gripping of contoured objects. The debut of SHRPS promises to significantly expand the versatile applications of e-skins.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ha24shrps</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stretchable hybrid response pressure sensors}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Matter}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1895-1908}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2590-2385}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.matt.2024.04.009}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2590238524001644}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ha, Kyoung-Ho and Li, Zhengjie and Kim, Sangjun and Huh, Heeyong and Wang, Zheliang and Shi, Hongyang and Block, Charles and Bhattacharya, Sarnab and Lu, Nanshu}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{stretchable pressure sensor, electronic skin, decoupling of pressure and stretch, inflatable soft robots, human-computer interaction, heartbeat palpation}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#00aa00">
            
              <a href="https://www.asplos-conference.org/" rel="external nofollow noopener" target="_blank">ASPLOS</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="block24twoface" class="col-sm-8">
    <!-- Title -->
    <div class="title">Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Charles Block</em>, <a href="https://gergerog.github.io/" rel="external nofollow noopener" target="_blank">Gerasimos Gerogiannis</a>, <a href="https://charithmendis.com/" rel="external nofollow noopener" target="_blank">Charith Mendis</a>, <a href="https://scholar.google.com/citations?user=9jK5lfsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ariful Azad</a>, and <a href="https://iacoma.cs.uiuc.edu/josep/torrellas.html" rel="external nofollow noopener" target="_blank">Josep Torrellas</a>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, La Jolla, CA, USA,  Apr 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
      
      
      
        
          <a href="/assets/pdf/asplos24_3.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Sparse matrix dense matrix multiplication (SpMM) is commonly used in applications ranging from scientific computing to graph neural networks. Typically, when SpMM is executed in a distributed platform, communication costs dominate. Such costs depend on how communication is scheduled. If it is scheduled in a sparsity-unaware manner, such as with collectives, execution is often inefficient due to unnecessary data transfers. On the other hand, if communication is scheduled in a fine-grained sparsity-aware manner, communicating only the necessary data, execution can also be inefficient due to high software overhead.We observe that individual sparse matrices often contain regions that are denser and regions that are sparser. Based on this observation, we develop a model that partitions communication into sparsity-unaware and sparsity-aware components. Leveraging the partition, we develop a new algorithm that performs collective communication for the denser regions, and fine-grained, one-sided communication for the sparser regions. We call the algorithm Two-Face. We show that Two-Face attains an average speedup of 2.11x over prior work when evaluated on a 4096-core supercomputer. Additionally, Two-Face scales well with the machine size.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">block24twoface</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Block, Charles and Gerogiannis, Gerasimos and Mendis, Charith and Azad, Ariful and Torrellas, Josep}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703850}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3620665.3640427}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3620665.3640427}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1200–1217}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{high-performance computing, distributed algorithms, sparse matrices, SpMM}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{La Jolla, CA, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ASPLOS '24}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>

</div>

  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2026
      Chase
      
      Block. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-S3WKFCL66D"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      window.dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-S3WKFCL66D');
  </script>




    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
